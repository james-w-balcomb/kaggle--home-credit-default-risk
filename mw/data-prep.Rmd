---
title: "analysis"
author: "mwinkler"
date: "July 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/matt.winkler/Documents/repos/kaggle--home-credit-default-risk/mw')
```


### Load original training data and modified train_test set:
```{r}
tr <- read.csv("../data/application_train.csv")
tr_te <- read.csv("../data/train_test_combined.csv")

```

### Split training and test datasets:

```{r}
tri <- 1:nrow(tr)
tr$TARGET <- as.factor(tr$TARGET)
y <- tr$TARGET

tr <- tr_te[tri, ]
te <- tr_te[-tri, ]

rm(tr_te)
```

### Check for missing values:

```{r}
my.fun <- function(vec) {
  na.ind = is.na(vec)
  ct = length(vec)
  na.tot = sum(na.ind)
  na.tot / ct
}

missing <- sapply(tr, my.fun)

qplot(missing,
      geom="histogram",
      bins = 30,  
      main = "Distribution of missing value %s", 
      xlab = "% Missing",
      ylab = "Num Variables",
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2))
```

We know there are issues with missing values in the data. Evaluate which features are good predictors of the target before fililng in their missing values. The below uses the Information Value calculated in the Information package. By default, Information Value calculates scores for bins of each feature, including a NULL group. Sometimes, a NULL group may itself be informative for prediction. However, it may also be the case that 

```{r}
library(Information)
tr$TARGET <- as.numeric(y)
tr$TARGET[y == 1] <- 0
tr$TARGET[y == 2] <- 1

results <- data.frame(names(tr[, 3:856]))
names(results) <- "variable"
row.names(results) <- results$variable
results$IV <- as.numeric(0.00)

# find information value scores for non-null instances by variable
for (v in names(tr[, 3:856])) {
  new.data <- tr[, c(v, "TARGET")]
  new.data <- new.data[complete.cases(new.data),]
  
  # some variables have only 1 unique value: 
  iv <- try(create_infotables(data = new.data, y = "TARGET"), 0.00)
  
  if (class(iv) != "Information") {
    results[v, ]$IV <- 0.00
  } else {
  
  iv <- iv$Summary$IV
  results[v,]$IV <- iv
  }
}

qplot(results$IV,
      geom="histogram",
      bins = 30,  
      main = "Distribution Information Values", 
      xlab = "IV Score (Non-Null)",
      ylab = "Count",
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2))

```

### Examine combination of NULLs and Variable importances:

```{r}
missing <- data.frame(missing)
missing$variable <- row.names(missing)

results <- left_join(results,
            missing,
            by = 'variable')

ggplot(results, aes(x=IV, y=missing)) +
  geom_point() +
  geom_rug()

```

Some of the more important variables (X axis) are also missing the most data. Not ideal!

### Select strong predictors:

Before doing the work to fill in missing values in the data 

```{r}
selected <- results[which(results$IV >= .05),]
tr.selected <- tr[, selected$variable]
tr.selected <- tr.selected[, 3:163]
ncol(tr.selected)
```





```{r}


tr.imputed <- missForest(tr.selected, mtry=5, ntree=10)
tr.imputed$OOBerror

#tr.complete <- complete(tr.imputed, 2)

```




# downsample main dataset before running xgboost, since it takes a long time otherwise.
set.seed(1234)

```{r}
down_tr <- caret::downSample(x = tr[, -1],
                         y = y)

y_tr <- as.numeric(down_tr$Class)
y_tr[y_tr == 1] <- 0
y_tr[y_tr == 2] <- 1

down_tr <- down_tr[, -856] %>% as.matrix()

```
`

```{r}


tri <- caret::createDataPartition(y_tr, p = 0.9, list = F) %>% c()
dtrain <- xgb.DMatrix(data = down_tr[tri,], label = y_tr[tri])
dval <- xgb.DMatrix(data = down_tr[-tri,], label = y_tr[-tri])
cols <- colnames(down_tr)

rm(down_tr, tr, te, tr_te, tri); gc()

params <- list(objective = "binary:logistic",
          booster = "gbtree",
          eval_metric = "auc",
          nthread = 4,
          eta = 0.05,
          max_depth = 6,
          min_child_weight = 30,
          gamma = 0,
          subsample = 0.85,
          colsample_bytree = 0.7,
          colsample_bylevel = 0.632,
          alpha = 0,
          lambda = 0,
          nrounds = 500)

set.seed(1234)
m_xgb <- xgb.train(params, 
                   dtrain, 
                   params$nrounds, 
                   list(val = dval), 
                   print_every_n = 50, 
                   early_stopping_rounds = 300)

```


#### Inspect most important features:
```{r}

cols <- colnames(down_tr)


# inspect the top features:
xgb.importance(feature_names=cols, model=m_xgb) %>% 
  xgb.plot.importance(top_n = 50)

imp <- xgb.importance(cols, model=m_xgb)
imp
```


Information gain drops off very quickly.
```{r}
plot(imp$Gain[1:200], type='l')
#plot(imp$Cover[1:200], type='l')
#plot(imp$Frequency[1:200], type='l')
```

Subselect features based on information gain for further analysis:
```{r}
selected <- imp %>% filter(Gain >= .001)

selected.X <- tr_te[, selected$Feature]
tri <- 1:nrow(tr)
selected.X <- selected.X[tri,]
```
